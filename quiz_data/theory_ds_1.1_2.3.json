{
  "quiz_title": "Linear models - SVM)",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему SVM с линейным ядром может работать лучше RBF на данных с 1000+ признаков?",
      "question_type": "single_choice",
      "options": [
        "Потому что RBF всегда переобучается",
        "Высокомерные данные чаще линейно разделимы (теорема Ковера)",
        "Линейное ядро автоматически выбирает важные признаки",
        "RBF не поддерживает sparse-данные"
      ],
      "correct_answer": 1,
      "explanation": "В пространствах высокой размерности (признаков >> наблюдений) данные становятся линейно разделимыми с высокой вероятностью (теорема Ковера), поэтому сложные ядра избыточны."
    },
    {
      "question_id": 2,
      "question_text": "Какое утверждение о параметре C в SVM НЕверно?",
      "question_type": "single_choice",
      "options": [
        "C контролирует компромисс между шириной разделяющей полосы и числом ошибок",
        "Малые значения C увеличивают margin, но допускают больше misclassified точек",
        "C=∞ означает жесткий margin без допущения ошибок",
        "Оптимальное C не зависит от масштабирования признаков"
      ],
      "correct_answer": 3,
      "explanation": "C критически зависит от масштаба данных. При масштабировании признаков (например, StandardScaler) оптимальное значение C может сильно измениться."
    },
    {
      "question_id": 3,
      "question_text": "Как gamma в RBF-ядре влияет на границу решения при увеличении?",
      "question_type": "single_choice",
      "options": [
        "Граница становится более гладкой, игнорируя выбросы",
        "Каждая опорная точка получает локальную область влияния",
        "Граница стремится к линейной",
        "SVM начинает учитывать только ближайшие k соседей"
      ],
      "correct_answer": 1,
      "explanation": "gamma = 1/(2σ²): при увеличении gamma радиус влияния каждой опорной точки уменьшается → граница становится «рваной», точно повторяя контуры тренировочных данных (риск overfit)."
    },
    {
      "question_id": 4,
      "question_text": "Почему SVM плохо масштабируется на больших датасетах?",
      "question_type": "single_choice",
      "options": [
        "Из-за необходимости хранить матрицу ядер размера O(n²)",
        "Отсутствия поддержки sparse-матриц",
        "Ограничения только бинарной классификацией",
        "Невозможности распараллеливания"
      ],
      "correct_answer": 0,
      "explanation": "SVM требует вычисления и хранения попарных ядерных функций между всеми точками (матрица Грама), что требует O(n²) памяти. Решения: алгоритмы типа SMO, мини-батчи или LIBSVM."
    },
    {
      "question_id": 5,
      "question_text": "Какое преимущество SVC(kernel='precomputed')?",
      "question_type": "single_choice",
      "options": [
        "Позволяет использовать пользовательские ядерные функции",
        "Автоматически выбирает оптимальное ядро",
        "Ускоряет обучение в 10 раз",
        "Поддерживает только линейные ядра"
      ],
      "correct_answer": 0,
      "explanation": "Режим 'precomputed' позволяет передать заранее вычисленную матрицу ядер, что полезно для кастомных ядер или когда ядро вычисляется внешней библиотекой."
    },
    {
      "question_id": 6,
      "question_text": "Чем отличается NuSVC от обычного SVC?",
      "question_type": "single_choice",
      "options": [
        "Nu параметризует верхнюю границу доли опорных векторов",
        "Поддерживает только квадратичное ядро",
        "Не требует масштабирования данных",
        "Автоматически подбирает параметр C"
      ],
      "correct_answer": 0,
      "explanation": "Nu ∈ (0,1] контролирует компромисс между долей опорных векторов и ошибками (аналог C, но с более интуитивной интерпретацией). Nu ≈ доля точек, становящихся support vectors."
    },
    {
      "question_id": 7,
      "question_text": "Какие из этих методов НЕ помогают ускорить SVM на больших данных? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Использование LinearSVC вместо SVC(kernel='linear')",
        "Увеличение параметра cache_size",
        "Применение PCA со снижением до 2 признаков",
        "Использование RBF-ядра с gamma='auto'",
        "Выбор маленького значения tol (точности оптимизации)"
      ],
      "correct_answers": [2, 4],
      "explanation": "PCA до 2 признаков радикально теряет информацию. Малое tol УВЕЛИЧИВАЕТ время обучения (более точная сходимость). LinearSVC реализован через liblinear (быстрее libsvm), cache_size уменьшает дисковые операции, auto-gamma адаптируется к числу признаков."
    },
    {
      "question_id": 8,
      "question_text": "Какие утверждения о SVR (Support Vector Regression) верны? (Выберите 3)",
      "question_type": "multi_choice",
      "options": [
        "Использует ε-трубку для допущения ошибок",
        "Чувствителен к выбросам из-за квадратичных потерь",
        "Параметр epsilon контролирует ширину нечувствительной зоны",
        "Всегда возвращает кусочно-линейную функцию",
        "Поддерживает те же ядра, что и SVC"
      ],
      "correct_answers": [0, 2, 4],
      "explanation": "SVR минимизирует L2-норму весов при условии попадания предсказаний в ε-трубку (аналог margin в SVC). Нечувствителен к выбросам (ε-трубка + hinge loss). Может быть нелинейным при использовании ядер (RBF/poly)."
    },
    {
      "question_id": 9,
      "question_text": "Почему стандартный SVM не поддерживает мультиклассовую классификацию «из коробки»?",
      "question_type": "single_choice",
      "options": [
        "Это ограничение математического аппарата",
        "Оптимизационная задача становится невыпуклой",
        "Реализованы только one-vs-rest и one-vs-one стратегии",
        "Невозможно определить margin для нескольких классов"
      ],
      "correct_answer": 2,
      "explanation": "SVM изначально бинарный классификатор. В sklearn мультикласс реализуется через: 1) One-vs-Rest (1 модель на класс) 2) One-vs-One (k(k-1)/2 моделей). Выбор стратегии влияет на качество при несбалансированных классах."
    },
    {
      "question_id": 10,
      "question_text": "Какие особенности SVM в R (e1071) по сравнению с Python (sklearn)? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Параметр cost в R аналог C в Python",
        "Только R поддерживает пользовательские ядра",
        "В R по умолчанию scale=TRUE (автомасштабирование)",
        "sklearn быстрее на больших данных",
        "R не поддерживает SVR"
      ],
      "correct_answers": [0, 2],
      "explanation": "cost в R = C в Python (штраф за ошибки). e1071 автоматически масштабирует данные (scale=TRUE), что может неожиданно изменить результаты. Обе реализации поддерживают SVR и кастомные ядра, но sklearn оптимизирован для производительности."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1o4oq2rY_4a-NxzlCHx78otq7akg42TFX/view?usp=sharing",
    "https://drive.google.com/file/d/1tGdjNIJ16dFLoMXcWrcNoEpENgly2-BQ/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["SVM", "Python", "R", "kernel_trick", "hyperparameters", "scaling", "SVR", "multiclass"],
    "author": "ML Expert",
    "created_at": "2023-11-20"
  }
}