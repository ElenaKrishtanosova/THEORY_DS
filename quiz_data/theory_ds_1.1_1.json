{
  "quiz_title": "Basic understanding of the most common algorithms",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему XGBoost часто превосходит Random Forest на табличных данных?",
      "question_type": "single_choice",
      "options": [
        "Потому что использует только линейные модели",
        "Из-за комбинации бустинга, регуляризации и обработки пропусков",
        "Он всегда работает быстрее из-за меньшего числа деревьев",
        "Не требует подбора гиперпараметров"
      ],
      "correct_answer": 1,
      "explanation": "XGBoost выигрывает за счет: 1) Последовательного исправления ошибок (бустинг vs бэггинг в RF) 2) L1/L2-регуляризации 3) Встроенной обработки пропусков 4) Оптимизированного вычисления gain при разбиениях"
    },
    {
      "question_id": 2,
      "question_text": "Какая фундаментальная разница в разбиении деревьев между LightGBM и классическим GBDT?",
      "question_type": "single_choice",
      "options": [
        "LightGBM всегда использует только бинарные разбиения",
        "Leaf-wise vs level-wise рост деревьев",
        "LightGBM игнорирует категориальные признаки",
        "В LightGBM нельзя контролировать глубину"
      ],
      "correct_answer": 1,
      "explanation": "LightGBM: leaf-wise (выбирает лист с max loss для роста) → точнее, но риск переобучения. Классический GBDT: level-wise (растёт весь уровень) → устойчивее, но менее точен"
    },
    {
      "question_id": 3,
      "question_text": "Когда K-mean++ даст значимое преимущество перед стандартным K-means?",
      "question_type": "single_choice",
      "options": [
        "При кластеризации идеально сферических данных",
        "При наличии выбросов и неравномерной плотности кластеров",
        "Когда число кластеров K < 3",
        "На линейно разделимых данных"
      ],
      "correct_answer": 1,
      "explanation": "K-means++ улучшает инициализацию центроидов за счёт вероятностного выбора, что уменьшает влияние выбросов и 'застревание' в локальных оптимумах"
    },
    {
      "question_id": 4,
      "question_text": "Почему в SVM с RBF-ядром gamma=0.1 может работать хуже, чем gamma=0.01 на больших признаковых пространствах?",
      "question_type": "single_choice",
      "options": [
        "Большее gamma всегда дает лучший результат",
        "Высокое gamma приводит к переобучению на шумных признаках",
        "SVM нечувствителен к параметру gamma",
        "При gamma>0.1 ядро перестаёт быть положительно определённым"
      ],
      "correct_answer": 1,
      "explanation": "gamma = 1/(2σ²): большое gamma → узкие 'колоколы' → модель учитывает мелкие шумы (риск overfit), малое gamma → широкие 'колоколы' → сглаживание границы"
    },
    {
      "question_id": 5,
      "question_text": "Какое утверждение о CatBoost НЕверно?",
      "question_type": "single_choice",
      "options": [
        "Автоматически обрабатывает категориальные признаки без one-hot encoding",
        "Использует oblivious trees (симметричные деревья)",
        "Требует обязательной нормализации числовых признаков",
        "Поддерживает GPU-ускорение"
      ],
      "correct_answer": 2,
      "explanation": "CatBoost не требует предварительной нормализации (в отличие от NN/SVM), самостоятельно кодирует категории через Ordered Target Statistics"
    },
    {
      "question_id": 6,
      "question_text": "Почему в Random Forest mtry=sqrt(p) (p — число признаков) часто лучше, чем mtry=p?",
      "question_type": "single_choice",
      "options": [
        "Уменьшает корреляцию между деревьями, повышая diversity ансамбля",
        "Ускоряет обучение в 2 раза",
        "Гарантирует включение всех признаков в каждое дерево",
        "Устраняет необходимость кросс-валидации"
      ],
      "correct_answer": 0,
      "explanation": "mtry=sqrt(p) уменьшает корреляцию ошибок за счёт разных подмножеств признаков в деревьях и более 'разнообразных' решающих правил"
    },
    {
      "question_id": 7,
      "question_text": "Какая особенность Hamming distance делает её полезной в kNN для текстов?",
      "question_type": "single_choice",
      "options": [
        "Чувствительность к абсолютным значениям признаков",
        "Инвариантность к перестановкам бинарных векторов",
        "Способность работать только с непрерывными переменными",
        "Линейная зависимость от длины векторов"
      ],
      "correct_answer": 1,
      "explanation": "Hamming distance считает число несовпадающих битов в бинарных векторах и идеальна для бинарного кодирования текстов (не зависит от порядка слов)"
    },
    {
      "question_id": 8,
      "question_text": "Какой алгоритм вы бы выбрали для прогнозирования временного ряда с 10K+ признаками и почему?",
      "question_type": "single_choice",
      "options": [
        "Линейную регрессию с L1-регуляризацией",
        "Дерево решений глубиной 10",
        "LightGBM с lags-признаками",
        "K-means кластеризацию"
      ],
      "correct_answer": 2,
      "explanation": "LightGBM эффективен на высокомерных данных (автоматический отбор признаков), поддерживает временные признаки (lags, rolling stats) и быстрее XGBoost"
    },
    {
      "question_id": 9,
      "question_text": "Какие из следующих утверждений о регуляризации в линейных моделях НЕверны? (Выберите 2 варианта)",
      "question_type": "multi_choice",
      "options": [
        "L1-регуляризация (Lasso) может обнулять коэффициенты",
        "L2-регуляризация (Ridge) увеличивает дисперсию модели",
        "Регуляризация всегда улучшает accuracy на тренировочных данных",
        "ElasticNet комбинирует L1 и L2, но требует ручной настройки смешивания"
      ],
      "correct_answers": [1, 2],
      "explanation": "L2-регуляризация НЕ увеличивает дисперсию — она уменьшает её. Регуляризация может ухудшить accuracy на тренировочных данных (но улучшить на тестовых). Правильные утверждения: Lasso действительно обнуляет коэффициенты, а ElasticNet автоматически настраивает баланс L1/L2."
    },
    {
      "question_id": 10,
      "question_text": "В каких случаях Decision Tree будет давать существенно разные результаты при небольших изменениях данных? (Выберите 3 варианта)",
      "question_type": "multi_choice",
      "options": [
        "При использовании Gini impurity вместо Information Gain",
        "Если разбиение по ключевому признаку меняет порядок",
        "Когда в узле остаётся мало наблюдений",
        "При наличии мультиколлинеарности",
        "Если максимальная глубина дерева установлена в 3"
      ],
      "correct_answers": [1, 2, 4],
      "explanation": "Деревья нестабильны когда: 1) Ключевые разбиения меняются (чувствительность к данным) 2) В узлах мало данных (шум влияет сильнее) 3) При малой глубине (каждое разбиение критично). Gini vs IG дают схожие результаты, а мультиколлинеарность не так критична для деревьев."
    },
    {
      "question_id": 11,
      "question_text": "Какие из этих методов НЕ являются решениями проблемы переобучения в Supervised Learning? (Выберите 2 варианта)",
      "question_type": "multi_choice",
      "options": [
        "Увеличение размера тренировочного набора",
        "Добавление случайного шума к входным данным",
        "Использование более сложной архитектуры модели",
        "Применение кросс-валидации вместо train-test split",
        "Ручное удаление выбросов из данных"
      ],
      "correct_answers": [2, 4],
      "explanation": "Усложнение модели ВЫЗЫВАЕТ переобучение, а удаление выбросов решает проблему outliers, но не overfitting. Правильные методы борьбы с overfitting: 1) Больше данных 2) Добавление шума (аугментация) 3) Кросс-валидация."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1b1c2KDl5JjmwD9kdkIOeYOXIZilBL9Hj/view?usp=sharing",
    "https://drive.google.com/file/d/1v7xhxlQnsSfhnnZOyTIYNw0H4yv5FYXZ/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["XGBoost", "LightGBM", "SVM", "Clustering", "CatBoost", "regularization", "decision_trees", "overfitting"],
    "author": "AI Tutor",
    "created_at": "2023-11-15"
  }
}