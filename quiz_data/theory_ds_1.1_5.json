{
  "quiz_title": "Linear Discriminant Analysis (LDA)",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему в LDA максимальное число дискриминантных компонент равно C-1, где C - количество классов?",
      "question_type": "single_choice",
      "options": [
        "Из-за ограничений вычислительной мощности",
        "Матрица междуклассового разброса Sb имеет ранг не более C-1",
        "Это произвольное ограничение алгоритма",
        "Чтобы избежать переобучения"
      ],
      "correct_answer": 1,
      "explanation": "Матрица Sb является суммой C матриц ранга 1 (для каждого класса), поэтому её максимальный ранг C-1. Это ограничивает число ненулевых собственных значений."
    },
    {
      "question_id": 2,
      "question_text": "Какие из следующих предположений делает LDA? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Признаки имеют нормальное распределение внутри каждого класса",
        "Все классы имеют одинаковую ковариационную матрицу",
        "Признаки независимы друг от друга",
        "Классы сбалансированы"
      ],
      "correct_answers": [0, 1],
      "explanation": "LDA предполагает: 1) нормальность распределения признаков внутри классов, 2) равенство ковариационных матриц. На практике LDA часто работает и при нарушении этих предположений."
    },
    {
      "question_id": 3,
      "question_text": "Как LDA обрабатывает проблему малой выборки (когда число признаков > числа наблюдений)?",
      "question_type": "single_choice",
      "options": [
        "Использует регуляризацию через добавление λI к Sw",
        "Автоматически уменьшает число признаков",
        "Переходит к нелинейной версии алгоритма",
        "LDA принципиально не работает в таких условиях"
      ],
      "correct_answer": 0,
      "explanation": "При проблеме SSS (small sample size) добавляют регуляризационный параметр λ к диагонали матрицы Sw, делая её обратимой: Sw + λI."
    },
    {
      "question_id": 4,
      "question_text": "В чём ключевое отличие LDA от PCA для снижения размерности?",
      "question_type": "single_choice",
      "options": [
        "PCA максимизирует межклассовый разброс, LDA - общую дисперсию",
        "LDA - supervised метод, использующий метки классов",
        "PCA всегда даёт лучшее качество классификации",
        "LDA может работать только с бинарной классификацией"
      ],
      "correct_answer": 1,
      "explanation": "Главное отличие: LDA использует информацию о классах (supervised), максимизируя отношение межклассового разброса к внутриклассовому, PCA же (unsupervised) максимизирует общую дисперсию."
    },
    {
      "question_id": 5,
      "question_text": "Почему LDA может плохо работать для нелинейно разделимых классов?",
      "question_type": "single_choice",
      "options": [
        "Он использует только линейные комбинации признаков",
        "Не учитывает информацию о классах",
        "Слишком чувствителен к выбросам",
        "Требует равенства ковариационных матриц"
      ],
      "correct_answer": 0,
      "explanation": "LDA строит линейные границы решений. Для нелинейных случаев можно использовать Kernel LDA или QDA (Quadratic Discriminant Analysis)."
    },
    {
      "question_id": 6,
      "question_text": "Какие из методов являются альтернативами LDA для нелинейного случая? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Quadratic Discriminant Analysis (QDA)",
        "Logistic Regression",
        "Kernel PCA",
        "Linear Regression"
      ],
      "correct_answers": [0, 2],
      "explanation": "QDA ослабляет предположение о равенстве ковариационных матриц, Kernel LDA/PCA используют kernel trick для нелинейных преобразований."
    },
    {
      "question_id": 7,
      "question_text": "Как вычисляется матрица внутриклассового разброса Sw в LDA?",
      "question_type": "single_choice",
      "options": [
        "Сумма ковариационных матриц всех классов",
        "Ковариационная матрица всех данных",
        "Разница между общей и межклассовой матрицами",
        "Внешнее произведение векторов средних"
      ],
      "correct_answer": 0,
      "explanation": "Sw = Σ Σ (x - μi)(x - μi)^T по всем классам i и точкам x класса i, что эквивалентно сумме ковариационных матриц классов."
    },
    {
      "question_id": 8,
      "question_text": "Что произойдёт, если в LDA классы имеют сильно различающиеся ковариационные матрицы?",
      "question_type": "single_choice",
      "options": [
        "Качество классификации существенно ухудшится",
        "LDA автоматически перейдёт в QDA",
        "Нужно использовать Regularized LDA",
        "Ничего, LDA устойчив к этому"
      ],
      "correct_answer": 0,
      "explanation": "Нарушение предположения о равенстве ковариационных матриц ведёт к снижению качества. В этом случае лучше использовать QDA."
    },
    {
      "question_id": 9,
      "question_text": "Как в LDA вычисляются posterior probabilities для классификации?",
      "question_type": "single_choice",
      "options": [
        "Через теорему Байеса, используя нормальное распределение",
        "Методом k-ближайших соседей",
        "Численным интегрированием",
        "Через логистическую функцию"
      ],
      "correct_answer": 0,
      "explanation": "P(Y=k|X=x) вычисляется по Байесу, где likelihood P(X=x|Y=k) задаётся многомерным нормальным распределением."
    },
    {
      "question_id": 10,
      "question_text": "Какое из утверждений о LDA и QDA верно?",
      "question_type": "single_choice",
      "options": [
        "QDA - частный случай LDA",
        "LDA имеет квадратичную границу решения",
        "QDA требует больше параметров для оценки",
        "LDA всегда даёт лучшую точность"
      ],
      "correct_answer": 2,
      "explanation": "QDA оценивает отдельную ковариационную матрицу для каждого класса (больше параметров). LDA - частный случай QDA с общей ковариационной матрицей."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1I3xQVAv3KKblFji_uuCL6D14ks9Vkc-f/view?usp=sharing",
    "https://drive.google.com/file/d/1jFkApqPLMVvhSUWxo1FkrZuUaY1vJSat/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["lda", "dimensionality_reduction", "classification", "bayes", "covariance"],
    "author": "ML Expert",
    "created_at": "2024-02-10"
  }
}