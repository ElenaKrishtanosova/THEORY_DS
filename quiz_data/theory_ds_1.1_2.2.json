{
  "quiz_title": "Linear models - Logistic regression",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему MSE не используется как функция потерь в логистической регрессии?",
      "question_type": "single_choice",
      "options": [
        "MSE не дифференцируема",
        "Приводит к невыпуклой функции потерь, затрудняя оптимизацию",
        "Логистическая регрессия требует только точных классификаций",
        "MSE слишком чувствительна к выбросам"
      ],
      "correct_answer": 1,
      "explanation": "MSE создаёт невыпуклый ландшафт ошибок с множеством локальных минимумов, что делает градиентный спуск неэффективным. Функция log loss сохраняет выпуклость."
    },
    {
      "question_id": 2,
      "question_text": "Какое утверждение о сигмоидной функции НЕверно?",
      "question_type": "single_choice",
      "options": [
        "Преобразует любые входные значения в диапазон (0,1)",
        "Её производная может быть выражена через саму функцию: σ' = σ(1-σ)",
        "При z=0 значение функции равно 0.5",
        "Является монотонно убывающей функцией"
      ],
      "correct_answer": 3,
      "explanation": "Сигмоидная функция монотонно ВОЗРАСТАЕТ (чем больше z, тем ближе к 1). Все остальные свойства верны."
    },
    {
      "question_id": 3,
      "question_text": "Как изменится предсказанная вероятность при увеличении коэффициента β₁ в 2 раза? P(Y=1) = σ(β₀ + β₁X₁)",
      "question_type": "single_choice",
      "options": [
        "Кривая станет круче, но точка P=0.5 останется при том же X₁",
        "Точка P=0.5 сместится влево/вправо в зависимости от знака β₁",
        "Функция станет несимметричной относительно 0.5",
        "Никак не изменится"
      ],
      "correct_answer": 0,
      "explanation": "β₁ контролирует крутизну сигмоиды. Удвоение коэффициента делает переход от 0 к 1 более резким, но P=0.5 остаётся при X₁ = -β₀/β₁."
    },
    {
      "question_id": 4,
      "question_text": "Какие из методов регуляризации применимы к логистической регрессии? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "L1-регуляризация (Lasso)",
        "Dropout",
        "L2-регуляризация (Ridge)",
        "Batch Normalization"
      ],
      "correct_answers": [0, 2],
      "explanation": "L1 и L2 добавляют штраф к функции потерь. Dropout и BatchNorm используются в нейросетях, но не в классической логистической регрессии."
    },
    {
      "question_id": 5,
      "question_text": "Что произойдёт при добавлении нового признака, сильно коррелирующего с существующим?",
      "question_type": "single_choice",
      "options": [
        "Улучшится интерпретируемость коэффициентов",
        "Увеличится дисперсия оценок коэффициентов",
        "Модель автоматически обнулит один из коэффициентов",
        "Функция потерь станет невыпуклой"
      ],
      "correct_answer": 1,
      "explanation": "Сильная мультиколлинеарность увеличивает стандартные ошибки оценок коэффициентов (растёт дисперсия), но не нарушает выпуклость."
    },
    {
      "question_id": 6,
      "question_text": "Почему логистическая регрессия может плохо работать на линейно разделимых данных?",
      "question_type": "single_choice",
      "options": [
        "Коэффициенты стремятся к бесконечности",
        "Теряет выпуклость функции потерь",
        "Не может достичь accuracy=1.0",
        "Требует явного указания порога"
      ],
      "correct_answer": 0,
      "explanation": "При идеальной разделимости MLE даёт неограниченно растущие коэффициенты (||β|| → ∞). Решение: регуляризация или ограничение числа итераций."
    },
    {
      "question_id": 7,
      "question_text": "Какие утверждения о log-odds верны? (Выберите 3)",
      "question_type": "multi_choice",
      "options": [
        "log(p/(1-p)) может принимать любые вещественные значения",
        "При p=0.9 log-odds равен ≈2.2",
        "Является канонической функцией связи для биномиального семейства",
        "Линейна относительно вероятностей p",
        "Используется в критерии останова обучения"
      ],
      "correct_answers": [0, 1, 2],
      "explanation": "log-odds: ln(0.9/0.1)≈2.2. Это нелинейное преобразование, не используемое для останова. Каноническая связь минимизирует дисперсию оценок."
    },
    {
      "question_id": 8,
      "question_text": "Как изменится предсказание при масштабировании признаков в 10 раз? P(Y=1) = σ(β₀ + β₁X₁)",
      "question_type": "single_choice",
      "options": [
        "Коэффициент β₁ уменьшится в 10 раз, предсказания не изменятся",
        "Предсказанная вероятность для всех точек станет 0.5",
        "Потребуется пересчёт оптимального порога классификации",
        "Ничего не изменится"
      ],
      "correct_answer": 0,
      "explanation": "Масштабирование признаков требует обратного масштабирования коэффициентов: β₁'X₁' = β₁'(10X₁) ⇒ β₁' = β₁/10. Порог 0.5 остаётся инвариантным."
    },
    {
      "question_id": 9,
      "question_text": "Какие методы отбора признаков специфичны для логистической регрессии? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "L1-регуляризация",
        "Feature Importance по Gini",
        "Тест Вальда (Wald test)",
        "Рекурсивное исключение признаков (RFE)"
      ],
      "correct_answers": [0, 2],
      "explanation": "L1 обнуляет коэффициенты. Тест Вальда оценивает значимость признаков через β/SE(β). Gini и RFE — общие методы, не специфичные для логит-моделей."
    },
    {
      "question_id": 10,
      "question_text": "Что произойдёт при использовании линейного ядра в SVM вместо логистической регрессии на одних данных?",
      "question_type": "single_choice",
      "options": [
        "SVM будет максимизировать margin, а LR — likelihood",
        "Обе модели дадут идентичные предсказания",
        "LR станет неинтерпретируемой",
        "SVM потеряет выпуклость функции потерь"
      ],
      "correct_answer": 0,
      "explanation": "Хотя обе модели строят линейные границы, SVM фокусируется на максимизации зазора (margin), а LR — на максимизации правдоподобия. Решения обычно различаются."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1zicdQvyS52YKsvTiarqs5qpRJwTAW9Ed/view?usp=sharing",
    "https://drive.google.com/file/d/1kpptKnuCNJKNnbzYgkNdASZz-e_a6sf2/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["logistic_regression", "classification", "maximum_likelihood", "regularization", "interpretability"],
    "author": "Data Science Interview Expert",
    "created_at": "2024-01-15"
  }
}