{
  "quiz_title": "Decision Trees (Ensembles - Random Forest)",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему Random Forest дает более точные предсказания, чем одиночное дерево решений?",
      "question_type": "single_choice",
      "options": [
        "За счет уменьшения bias (смещения)",
        "За счет уменьшения variance (разброса)",
        "За счет увеличения глубины каждого дерева",
        "За счет использования всех признаков в каждом дереве"
      ],
      "correct_answer": 1,
      "explanation": "RF уменьшает variance за счет бэггинга (усреднения предсказаний множества деревьев) и случайного выбора признаков, что делает деревья менее коррелированными. Bias при этом остается примерно таким же."
    },
    {
      "question_id": 2,
      "question_text": "Какие из следующих утверждений о OOB-оценке верны? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "OOB-оценка вычисляется на данных, не вошедших в бутстрап-выборку",
        "Требует создания отдельного validation set",
        "Эквивалентна leave-one-out кросс-валидации при n_estimators → ∞",
        "Всегда менее точна, чем 5-fold CV",
        "Использует примерно 37% данных для каждого дерева"
      ],
      "correct_answers": [0, 2],
      "explanation": "Out-of-Bag оценка использует ~37% неотобранных данных для валидации каждого дерева. При большом числе деревьев приближается к LOOCV. Не требует отдельного validation set."
    },
    {
      "question_id": 3,
      "question_text": "Как изменится важность признака X, если добавить сильно коррелированный с ним признак Y?",
      "question_type": "single_choice",
      "options": [
        "Важность X уменьшится пропорционально корреляции",
        "Важность X и Y разделится примерно пополам",
        "Важность X останется неизменной",
        "RF автоматически исключит один из признаков"
      ],
      "correct_answer": 1,
      "explanation": "Из-за случайного выбора признаков, коррелированные признаки конкурируют за место в разбиениях, деля свою важность. Это отличает RF от линейных моделей."
    },
    {
      "question_id": 4,
      "question_text": "Почему max_features=sqrt(n_features) часто работает лучше, чем использование всех признаков?",
      "question_type": "single_choice",
      "options": [
        "Уменьшает время обучения",
        "Увеличивает разнообразие деревьев",
        "Уменьшает глубину деревьев",
        "Позволяет использовать больше деревьев"
      ],
      "correct_answer": 1,
      "explanation": "Использование подмножества признаков увеличивает разнообразие деревьев (декорреляцию), что улучшает качество ансамбля. Оптимальное значение обычно sqrt(n_features) для классификации и n_features/3 для регрессии."
    },
    {
      "question_id": 5,
      "question_text": "Какие методы могут улучшить Random Forest на несбалансированных данных? (Выберите 3)",
      "question_type": "multi_choice",
      "options": [
        "Использование class_weight='balanced'",
        "Увеличение min_samples_leaf",
        "Применение SMOTE для бутстрап-выборок",
        "Уменьшение max_depth",
        "Использование критерия entropy вместо gini"
      ],
      "correct_answers": [0, 2, 3],
      "explanation": "Проблема дисбаланса решается: 1) взвешиванием классов, 2) оверсэмплингом минорного класса (SMOTE), 3) ограничением глубины деревьев. Критерий entropy/gini почти эквивалентен."
    },
    {
      "question_id": 6,
      "question_text": "В чем ключевое отличие важности признаков в RF и Permutation Importance?",
      "question_type": "single_choice",
      "options": [
        "Permutation Importance учитывает взаимодействия признаков",
        "RF Importance смещена в сторону категориальных признаков",
        "Permutation Importance работает только для линейных моделей",
        "В RF Importance основана на снижении accuracy"
      ],
      "correct_answer": 0,
      "explanation": "RF Importance = среднее снижение impurity (Джини/энтропии). Permutation Importance измеряет падение accuracy при перемешивании признака и учитывает взаимодействия признаков."
    },
    {
      "question_id": 7,
      "question_text": "Когда ExtraTrees предпочтительнее обычного Random Forest?",
      "question_type": "single_choice",
      "options": [
        "При очень большом числе признаков",
        "Для задач с шумными данными",
        "Когда важна интерпретируемость",
        "Если нужны точные вероятности"
      ],
      "correct_answer": 1,
      "explanation": "ExtraTrees используют случайные пороги разбиения вместо оптимальных, что уменьшает variance (но увеличивает bias) и лучше работает на зашумленных данных."
    },
    {
      "question_id": 8,
      "question_text": "Как влияет увеличение n_estimators на: 1) переобучение 2) время предсказания?",
      "question_type": "single_choice",
      "options": [
        "1) Увеличивает 2) Увеличивает",
        "1) Не влияет 2) Увеличивает",
        "1) Уменьшает 2) Не влияет",
        "1) Не влияет 2) Не влияет"
      ],
      "correct_answer": 1,
      "explanation": "Больше деревьев не приводят к переобучению (асимптотически сходятся), но линейно увеличивают время предсказания. Оптимальное число выбирают по плато на OOB-оценке."
    },
    {
      "question_id": 9,
      "question_text": "Какие параметры сильнее всего влияют на переобучение в RF? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "n_estimators",
        "max_depth",
        "min_samples_leaf", 
        "max_features",
        "n_jobs"
      ],
      "correct_answers": [1, 2],
      "explanation": "Глубина деревьев (max_depth) и минимальное число samples в листе (min_samples_leaf) контролируют переобучение. n_estimators и max_features влияют слабее, n_jobs - только на скорость."
    },
    {
      "question_id": 10,
      "question_text": "Почему Random Forest плохо экстраполирует за пределы обучающих данных?",
      "question_type": "single_choice",
      "options": [
        "Деревья дают константные предсказания вне обучающего диапазона",
        "Не работает с непрерывными признаками",
        "Требует нормализации данных",
        "Чувствителен к выбросам"
      ],
      "correct_answer": 0,
      "explanation": "Предсказание RF - среднее предсказаний деревьев, которые дают константные значения в конечных листьях. За пределом обучающего диапазона предсказания не меняются."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1_LIOLB0bUW4y7MebO8IpKv66kfcwACrj/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["random_forest", "bagging", "feature_importance", "hyperparameters", "extrapolation"],
    "author": "ML Expert",
    "created_at": "2024-01-25"
  }
}