{
  "quiz_title": "Regularisation - Ridge vs Lasso Regression",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему Lasso может полностью обнулять коэффициенты, а Ridge — только уменьшать их?",
      "question_type": "single_choice",
      "options": [
        "Из-за использования L1-нормы вместо L2-нормы в штрафном члене",
        "Ridge работает только с положительными коэффициентами",
        "Lasso использует вторые производные",
        "Потому что Ridge всегда дает лучшие результаты"
      ],
      "correct_answer": 0,
      "explanation": "L1-норма (сумма модулей) в Lasso создает 'острые углы' в пространстве решений, где коэффициенты могут точно обнулиться, в отличие от 'гладкой' L2-нормы в Ridge."
    },
    {
      "question_id": 2,
      "question_text": "Какой из методов предпочтителен при высокой мультиколлинеарности признаков?",
      "question_type": "single_choice",
      "options": [
        "Lasso, так как он отберет один из коррелированных признаков",
        "Ridge, так как он равномерно распределит веса",
        "Оба метода одинаково хороши",
        "Ни один, нужно использовать PCA"
      ],
      "correct_answer": 1,
      "explanation": "Ridge регрессия лучше справляется с мультиколлинеарностью, распределяя веса между коррелированными признаками, в то время как Lasso случайным образом выберет один из них."
    },
    {
      "question_id": 3,
      "question_text": "Что произойдет с коэффициентами при λ→∞? (Выберите 2 верных варианта)",
      "question_type": "multi_choice",
      "options": [
        "В Ridge все коэффициенты стремятся к нулю",
        "В Lasso все коэффициенты становятся нулевыми",
        "В Ridge коэффициенты выравниваются",
        "В Lasso останется ровно один ненулевой коэффициент",
        "Оба метода дадут одинаковые результаты"
      ],
      "correct_answers": [0, 1],
      "explanation": "При бесконечно большом λ штрафной член доминирует, зануляя все коэффициенты в обоих методах, но путь к этому разный: Ridge асимптотически приближается, Lasso может точно обнулить."
    },
    {
      "question_id": 4,
      "question_text": "Почему Elastic Net сочетает L1 и L2 регуляризации?",
      "question_type": "single_choice",
      "options": [
        "Чтобы одновременно отбирать признаки и работать с мультиколлинеарностью",
        "Для ускорения вычислений",
        "Чтобы избежать необходимости кросс-валидации",
        "Это дает всегда лучший результат, чем по отдельности"
      ],
      "correct_answer": 0,
      "explanation": "Elastic Net сочетает преимущества Lasso (отбор признаков через L1) и Ridge (устойчивость к мультиколлинеарности через L2), компенсируя недостатки каждого метода."
    },
    {
      "question_id": 5,
      "question_text": "Как влияет масштабирование признаков на Ridge/Lasso регрессию?",
      "question_type": "single_choice",
      "options": [
        "Не влияет, так как методы инвариантны к масштабу",
        "Критически важно, иначе штраф будет несправедливым",
        "Нужно только для Ridge",
        "Требуется только для категориальных признаков"
      ],
      "correct_answer": 1,
      "explanation": "Без масштабирования признаки с большим диапазоном будут неоправданно сильнее штрафоваться. Масштабирование гарантирует равный вклад каждого признака в штрафной член."
    },
    {
      "question_id": 6,
      "question_text": "Какое утверждение о геометрической интерпретации верно?",
      "question_type": "single_choice",
      "options": [
        "Rridge соответствует касанию эллипса RSS с кругом",
        "Lasso соответствует касанию с квадратом, повернутым на 45°",
        "Решение Ridge всегда лежит на осях",
        "Lasso никогда не дает точного нуля"
      ],
      "correct_answer": 0,
      "explanation": "В геометрической интерпретации Ridge регрессия находит точку касания эллипса RSS (линии постоянного значения RSS) с кругом (L2-ограничение), в то время как Lasso — с ромбом (L1-ограничение)."
    },
    {
      "question_id": 7,
      "question_text": "Какой метод предпочтителен, если известно, что лишь немногие признаки действительно важны?",
      "question_type": "single_choice",
      "options": [
        "Ridge, так как он сохранит все признаки",
        "Lasso, так как он обнулит неважные",
        "Оба одинаково хороши",
        "Линейная регрессия без регуляризации"
      ],
      "correct_answer": 1,
      "explanation": "Lasso естественным образом выполняет отбор признаков, обнуляя коэффициенты нерелевантных переменных, что идеально подходит для sparse данных."
    },
    {
      "question_id": 8,
      "question_text": "Почему в Ridge коэффициенты стремятся к нулю, но не достигают его?",
      "question_type": "single_choice",
      "options": [
        "Из-за квадратичной природы штрафа",
        "Это ошибка реализации",
        "Только при неправильной настройке λ",
        "Потому что L2-норма не дифференцируема в нуле"
      ],
      "correct_answer": 0,
      "explanation": "Квадратичный штраф в Ridge (L2) делает функцию стоимости гладкой, без 'углов', где коэффициенты могли бы точно обнулиться. Градиент плавно уменьшает коэффициенты асимптотически к нулю."
    },
    {
      "question_id": 9,
      "question_text": "Какое из утверждений о вычислительной сложности неверно?",
      "question_type": "single_choice",
      "options": [
        "Ridge имеет аналитическое решение",
        "Lasso требует более сложных алгоритмов типа coordinate descent",
        "Elastic Net вычислительно сложнее обоих",
        "Ridge нельзя использовать для данных с p >> n"
      ],
      "correct_answer": 3,
      "explanation": "Неверно, что Ridge нельзя использовать при p >> n (много признаков, мало примеров). Ridge часто применяется в таких случаях, в отличие от обычной линейной регрессии."
    },
    {
      "question_id": 10,
      "question_text": "Как изменяется эффективное число параметров при увеличении λ?",
      "question_type": "single_choice",
      "options": [
        "В Ridge уменьшается постепенно, в Lasso — скачками",
        "В Lasso увеличивается линейно",
        "В Ridge может резко упасть до нуля",
        "Оба метода дают одинаковую динамику"
      ],
      "correct_answer": 0,
      "explanation": "В Ridge 'эффективная' сложность модели (степени свободы) уменьшается плавно с ростом λ. В Lasso это происходит скачкообразно при обнулении каждого нового коэффициента."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1LQXP4QYLlcgUJopkOuFb3DG1qEG6L0sU/view?usp=sharing",
    "https://drive.google.com/file/d/17r0wDto-M9VwDAs4IhOm4C6SpUXmk7yd/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["Ridge", "Lasso", "Regularization", "L1", "L2", "Feature Selection"],
    "author": "ML Expert",
    "created_at": "2023-11-28"
  }
}