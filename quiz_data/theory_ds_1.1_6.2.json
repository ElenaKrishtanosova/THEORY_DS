{
  "quiz_title": "PCA: Technical Deep Dive",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему стандартизация данных критически важна перед применением PCA?",
      "question_type": "single_choice",
      "options": [
        "PCA работает только с бинарными данными",
        "Чтобы признаки с большими числовыми диапазонами не доминировали в компонентах",
        "Это ускоряет вычисление ковариационной матрицы",
        "PCA требует нормального распределения данных"
      ],
      "correct_answer": 1,
      "explanation": "Без стандартизации признаки с большими значениями (например, зарплата в тыс. долларов) будут искусственно получать больше веса в компонентах, чем признаки с малыми значениями (например, возраст)."
    },
    {
      "question_id": 2,
      "question_text": "Как интерпретировать собственные значения (eigenvalues) в PCA?",
      "question_type": "single_choice",
      "options": [
        "Они показывают долю дисперсии, объясняемую каждой компонентой",
        "Это веса исходных признаков в главных компонентах",
        "Количество ненулевых eigenvalues равно числу строк в данных",
        "Они всегда суммируются в 1"
      ],
      "correct_answer": 0,
      "explanation": "Собственные значения прямо пропорциональны доле дисперсии, которую объясняет соответствующая главная компонента. Их сумма равна общей дисперсии в данных."
    },
    {
      "question_id": 3,
      "question_text": "Что означает ортогональность главных компонент?",
      "question_type": "single_choice",
      "options": [
        "Они имеют нулевую корреляцию между собой",
        "Они содержат непересекающиеся исходные признаки",
        "Их собственные векторы параллельны",
        "Они всегда объясняют равные доли дисперсии"
      ],
      "correct_answer": 0,
      "explanation": "Ортогональность означает, что ковариация (и корреляция) между любыми двумя разными главными компонентами равна нулю — они статистически независимы."
    },
    {
      "question_id": 4,
      "question_text": "Какое утверждение о ковариационной матрице в PCA верно?",
      "question_type": "single_choice",
      "options": [
        "Её диагональные элементы — дисперсии признаков",
        "Она всегда диагональная",
        "Её след равен числу признаков",
        "Она не используется в PCA"
      ],
      "correct_answer": 0,
      "explanation": "На диагонали ковариационной матрицы находятся дисперсии отдельных признаков, вне диагонали — их попарные ковариации. PCA ищет её собственные векторы."
    },
    {
      "question_id": 5,
      "question_text": "Как выбрать оптимальное число главных компонент? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Правило Кайзера (eigenvalue > 1)",
        "Скри-график (elbow method)",
        "Максимизация accuracy на тестовых данных",
        "Всегда брать ровно половину исходных признаков",
        "Достижение 95% объяснённой дисперсии"
      ],
      "correct_answers": [0, 4],
      "explanation": "Основные методы: 1) Правило Кайзера — оставляем компоненты с eigenvalue > 1; 2) Анализ скри-графика (излом); 3) Кумулятивная дисперсия (обычно 90-95%)."
    },
    {
      "question_id": 6,
      "question_text": "Что произойдёт, если применить PCA к данным с мультиколлинеарностью?",
      "question_type": "single_choice",
      "options": [
        "PCA автоматически устранит линейно-зависимые признаки",
        "Получим вырожденную ковариационную матрицу",
        "Компоненты станут интерпретируемыми",
        "Ошибка 'division by zero'"
      ],
      "correct_answer": 0,
      "explanation": "PCA обнаружит признаки с корреляцией ~1 и создаст компоненты без избыточности — одна компонента заменит коррелированную группу, остальные получат нулевую дисперсию."
    },
    {
      "question_id": 7,
      "question_text": "Почему PCA плохо работает с категориальными признаками?",
      "question_type": "single_choice",
      "options": [
        "Он не может вычислять ковариацию для категорий",
        "Категории не имеют дисперсии",
        "Это нарушает предположение о линейных отношениях",
        "Все варианты верны"
      ],
      "correct_answer": 3,
      "explanation": "PCA опирается на ковариацию (линейную зависимость) и дисперсию. Категории требуют специального кодирования, а их природа часто нелинейна."
    },
    {
      "question_id": 8,
      "question_text": "Как связаны SVD и PCA для матрицы X?",
      "question_type": "single_choice",
      "options": [
        "Собственные векторы XᵀX — это правые сингулярные векторы X",
        "PCA — это SVD центрированной матрицы",
        "SVD не используется в PCA",
        "Они дают идентичные результаты только для квадратных матриц"
      ],
      "correct_answer": 1,
      "explanation": "Для центрированной матрицы X: PCA = SVD(X). Собственные векторы ковариационной матрицы (XᵀX) — это правые сингулярные векторы X."
    },
    {
      "question_id": 9,
      "question_text": "Какое преобразование обратимо при использовании PCA?",
      "question_type": "single_choice",
      "options": [
        "Уменьшение размерности (отбор компонент)",
        "Умножение на ортогональную матрицу",
        "Стандартизация данных",
        "Удаление шумовых компонент"
      ],
      "correct_answer": 1,
      "explanation": "Ортогональное преобразование (вращение) сохраняет все расстояния и информацию. Уменьшение размерности и удаление компонент теряет информацию безвозвратно."
    },
    {
      "question_id": 10,
      "question_text": "Как влияет выброс на результаты PCA?",
      "question_type": "single_choice",
      "options": [
        "Увеличивает дисперсию всех компонент",
        "Может радикально изменить направление главных компонент",
        "Не влияет, если данных много",
        "Только увеличивает время вычислений"
      ],
      "correct_answer": 1,
      "explanation": "PCA чувствителен к выбросам, так как максимизирует дисперсию. Один выброс может существенно изменить ковариационную матрицу и направление компонент."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1aqutpmoJmy6x_TuXtJa2WCEktbYZ0iNL/view?usp=sharing",
    "https://drive.google.com/file/d/1l9iaDy6Rhg4S8xffyW6A1h1Pk-fZ9ELS/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "intermediate",
    "tags": ["PCA", "Eigenvalues", "Covariance", "Dimensionality Reduction", "SVD"],
    "author": "PCA Expert",
    "created_at": "2023-11-25"
  }
}