{
  "quiz_title": "Dimensionality reduction (Basic understanding)",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему PCA теряет интерпретируемость признаков в отличие от Feature Selection?",
      "question_type": "single_choice",
      "options": [
        "PCA использует случайные проекции",
        "Преобразованные признаки — линейные комбинации исходных",
        "PCA удаляет все коррелированные признаки",
        "Из-за нормализации данных перед применением"
      ],
      "correct_answer": 1,
      "explanation": "Главные компоненты — взвешенные суммы исходных признаков (например, 0.3*Возраст + 0.7*Доход), что затрудняет их интерпретацию. Feature Selection сохраняет исходные смысл признаков."
    },
    {
      "question_id": 2,
      "question_text": "В каком случае t-SNE принципиально лучше PCA для визуализации?",
      "question_type": "single_choice",
      "options": [
        "При необходимости уменьшить размерность до 1D",
        "Для данных с нелинейными зависимостями",
        "Когда важно сохранить глобальную структуру данных",
        "Для быстрой обработки больших данных"
      ],
      "correct_answer": 1,
      "explanation": "t-SNE захватывает нелинейные структуры (кластеры) за счет вычисления парных сходств, но искажает глобальные расстояния. PCA — только линейные преобразования."
    },
    {
      "question_id": 3,
      "question_text": "Какое утверждение о LDA НЕверно?",
      "question_type": "single_choice",
      "options": [
        "Максимизирует расстояние между классами",
        "Требует размеченные данные для обучения",
        "Всегда дает лучшую проекцию чем PCA для классификации",
        "Чувствителен к выбросам в данных"
      ],
      "correct_answer": 2,
      "explanation": "LDA может проиграть PCA если дискриминантная информация содержится не в средних, а в дисперсиях (например, разные распределения внутри классов)."
    },
    {
      "question_id": 4,
      "question_text": "Почему UMAP часто предпочитают t-SNE для больших данных?",
      "question_type": "single_choice",
      "options": [
        "UMAP использует меньше памяти",
        "UMAP сохраняет глобальную структуру лучше",
        "UMAP не требует стандартизации данных",
        "UMAP работает только с категориальными признаками"
      ],
      "correct_answer": 1,
      "explanation": "UMAP оптимизирует как локальные, так и глобальные расстояния, имеет O(n) сложность против O(n²) у t-SNE, и лучше масштабируется."
    },
    {
      "question_id": 5,
      "question_text": "Какие методы НЕ подходят для текстовых данных в Bag-of-Words представлении? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "PCA с косинусной мерой",
        "t-SNE с евклидовой метрикой",
        "LDA без предварительной нормализации",
        "UMAP с косинусной мерой",
        "Feature Selection по дисперсии"
      ],
      "correct_answers": [1, 4],
      "explanation": "Для текстов критична косинусная метрика (евклидова плоха из-за разреженности). Отбор по дисперсии удалит редкие, но важные слова."
    },
    {
      "question_id": 6,
      "question_text": "Как изменится качество PCA если признаки измерены в разных единицах (например, возраст [годы] и доход [рубли]) без нормализации?",
      "question_type": "single_choice",
      "options": [
        "Улучшится, так как PCA любит разнородные данные",
        "Не изменится, если признаки независимы",
        "Ухудшится из-за разного масштаба признаков",
        "PCA вообще нельзя применять в таком случае"
      ],
      "correct_answer": 2,
      "explanation": "PCA чувствителен к масштабу — признаки с большим диапазоном (доход) получат искусственно высокую важность. Необходима стандартизация (Z-score)."
    },
    {
      "question_id": 7,
      "question_text": "Почему в Autoencoders для снижения размерности используют узкое горлышко (bottleneck)?",
      "question_type": "single_choice",
      "options": [
        "Чтобы увеличить число параметров модели",
        "Для принудительного сжатия информации",
        "Из-за ограничений GPU-памяти",
        "Это ускоряет обучение в 10 раз"
      ],
      "correct_answer": 1,
      "explanation": "Узкий слой в середине (например, 32 нейрона) вынуждает сеть выделять наиболее важные признаки для реконструкции данных."
    },
    {
      "question_id": 8,
      "question_text": "Какое утверждение о матрице ковариации в PCA верно?",
      "question_type": "single_choice",
      "options": [
        "Её диагональ содержит дисперсии признаков",
        "Она всегда диагональная",
        "Её след равен числу признаков",
        "Она не учитывается в PCA"
      ],
      "correct_answer": 0,
      "explanation": "Диагональ ковариационной матрицы — дисперсии отдельных признаков, вне диагонали — их ковариации. PCA находит её собственные векторы."
    },
    {
      "question_id": 9,
      "question_text": "Что произойдет, если применить PCA к данным с 100% корреляцией двух признаков?",
      "question_type": "single_choice",
      "options": [
        "Одна из главных компонент будет иметь нулевую дисперсию",
        "PCA выдаст ошибку деления на ноль",
        "Все компоненты сохранят одинаковую дисперсию",
        "Корреляция не влияет на работу PCA"
      ],
      "correct_answer": 0,
      "explanation": "PCA исключит избыточность — одна компонента будет копией коррелированной пары (ненулевая дисперсия), а вторая — шум (нулевая дисперсия)."
    },
    {
      "question_id": 10,
      "question_text": "Какие методы снижают размерность, сохраняя топологию данных? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "PCA",
        "t-SNE",
        "UMAP",
        "LDA",
        "Feature Selection по важности из Random Forest"
      ],
      "correct_answers": [1, 2],
      "explanation": "t-SNE и UMAP сохраняют локальные \"соседства\" точек. PCA и LDA — линейные методы, а Feature Selection вообще не учитывает геометрию данных."
    },
    {
      "question_id": 11,
      "question_text": "Почему NMF (Non-negative Matrix Factorization) популярен для текстов?",
      "question_type": "single_choice",
      "options": [
        "Он гарантирует неотрицательные веса слов в темах",
        "Он работает быстрее PCA в 100 раз",
        "Он автоматически удаляет стоп-слова",
        "Он не требует задания числа компонент"
      ],
      "correct_answer": 0,
      "explanation": "NMF дает интерпретируемые темы как суммы слов с положительными весами (в отличие от PCA, где компоненты — линейные комбинации с отрицательными коэффициентами)."
    },
    {
      "question_id": 12,
      "question_text": "Какое утверждение о Kernel PCA верно?",
      "question_type": "single_choice",
      "options": [
        "Он всегда линейно разделяет данные в новом пространстве",
        "Он использует ядровый трюк для нелинейных преобразований",
        "Его сложность всегда O(n³) независимо от ядра",
        "Он не требует выбора гиперпараметров"
      ],
      "correct_answer": 1,
      "explanation": "Kernel PCA применяет PCA в пространстве более высокой размерности через ядро (RBF, полиномиальное и др.), но требует выбора ядра и его параметров."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1u16Pq9n2OlXIgDKGzO1AuKLLFvIpInY6/view?usp=sharing",
    "https://drive.google.com/file/d/16KXtU4fgG53mf_z64Ar8ZWNJyrSTkXDf/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "advanced",
    "tags": ["PCA", "t-SNE", "LDA", "UMAP", "NMF", "Autoencoders", "Feature Selection", "Ковариация"],
    "author": "AI Tutor",
    "created_at": "2023-11-20"
  }
}