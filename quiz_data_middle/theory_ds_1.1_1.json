{
  "quiz_title": "Basic understanding of the most common algorithms",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Чем XGBoost отличается от Random Forest?",
      "question_type": "single_choice",
      "options": [
        "XGBoost строит деревья последовательно, исправляя ошибки предыдущих",
        "Random Forest всегда работает быстрее",
        "XGBoost использует только один случайный признак для каждого дерева",
        "В Random Forest нельзя настраивать глубину деревьев"
      ],
      "correct_answer": 0,
      "explanation": "XGBoost - это бустинг (деревья строятся последовательно), а Random Forest - бэггинг (деревья строятся независимо)"
    },
    {
      "question_id": 2,
      "question_text": "Какое главное преимущество LightGBM перед обычным GBDT?",
      "question_type": "single_choice",
      "options": [
        "Он растет деревья по уровням (level-wise)",
        "Он быстрее обучается на больших данных",
        "Он не поддерживает категориальные признаки",
        "Он всегда дает менее точные прогнозы"
      ],
      "correct_answer": 1,
      "explanation": "LightGBM оптимизирован для работы с большими данными и обычно обучается быстрее благодаря leaf-wise росту деревьев"
    },
    {
      "question_id": 3,
      "question_text": "Когда стоит использовать K-means++ вместо обычного K-means?",
      "question_type": "single_choice",
      "options": [
        "Когда данные идеально разделены на сферы",
        "Когда нужно улучшить начальный выбор центроидов",
        "Когда количество кластеров известно точно",
        "Когда все признаки имеют одинаковый масштаб"
      ],
      "correct_answer": 1,
      "explanation": "K-means++ улучшает начальную инициализацию центров кластеров, что помогает избежать плохих решений"
    },
    {
      "question_id": 4,
      "question_text": "Какое ядро SVM выбрать для линейно разделимых данных?",
      "question_type": "single_choice",
      "options": [
        "Линейное",
        "RBF (Гауссово)",
        "Полиномиальное 3-й степени",
        "Сигмоидное"
      ],
      "correct_answer": 0,
      "explanation": "Для линейно разделимых данных достаточно простого линейного ядра"
    },
    {
      "question_id": 5,
      "question_text": "Какое утверждение про CatBoost верно?",
      "question_type": "single_choice",
      "options": [
        "Требует обязательного one-hot кодирования категорий",
        "Автоматически обрабатывает категориальные признаки",
        "Работает только с числовыми данными",
        "Не поддерживает GPU"
      ],
      "correct_answer": 1,
      "explanation": "CatBoost умеет работать с категориальными признаками без предварительного кодирования"
    },
    {
      "question_id": 6,
      "question_text": "Почему в Random Forest используют случайные подмножества признаков?",
      "question_type": "single_choice",
      "options": [
        "Чтобы уменьшить корреляцию между деревьями",
        "Чтобы ускорить обучение в 10 раз",
        "Чтобы использовать все признаки одинаково",
        "Чтобы избежать переобучения на тестовых данных"
      ],
      "correct_answer": 0,
      "explanation": "Случайные подмножества признаков делают деревья более разнообразными и уменьшают корреляцию их ошибок"
    },
    {
      "question_id": 7,
      "question_text": "Какое расстояние используют для категориальных данных в kNN?",
      "question_type": "single_choice",
      "options": [
        "Евклидово расстояние",
        "Расстояние Хэмминга",
        "Манхэттенское расстояние",
        "Косинусное расстояние"
      ],
      "correct_answer": 1,
      "explanation": "Расстояние Хэмминга подходит для категориальных или бинарных данных, так как считает количество несовпадений"
    },
    {
      "question_id": 8,
      "question_text": "Какой алгоритм лучше для временных рядов?",
      "question_type": "single_choice",
      "options": [
        "K-means",
        "LightGBM с лагами",
        "Наивный Байес",
        "Линейная регрессия без регуляризации"
      ],
      "correct_answer": 1,
      "explanation": "LightGBM хорошо работает с временными рядами, особенно если добавить лаги (значения из прошлого) как признаки"
    },
    {
      "question_id": 9,
      "question_text": "Какие методы помогают бороться с переобучением? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Увеличение тренировочных данных",
        "Использование более сложной модели",
        "Регуляризация",
        "Уменьшение тестовой выборки",
        "Удаление всех выбросов"
      ],
      "correct_answers": [0, 2],
      "explanation": "Больше данных и регуляризация помогают против переобучения. Сложные модели наоборот могут его вызывать"
    },
    {
      "question_id": 10,
      "question_text": "Какие утверждения о деревьях решений верны? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Чувствительны к небольшим изменениям данных",
        "Всегда дают одинаковые результаты на похожих данных",
        "Не требуют масштабирования признаков",
        "Работают только с нормализованными данными",
        "Не могут обрабатывать категориальные признаки"
      ],
      "correct_answers": [0, 2],
      "explanation": "Деревья чувствительны к данным и не требуют масштабирования, так как работают с разбиениями, а не расстояниями"
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1b1c2KDl5JjmwD9kdkIOeYOXIZilBL9Hj/view?usp=sharing",
    "https://drive.google.com/file/d/1v7xhxlQnsSfhnnZOyTIYNw0H4yv5FYXZ/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "beginner",
    "tags": ["XGBoost", "LightGBM", "SVM", "Clustering", "CatBoost", "decision_trees"],
    "author": "AI Tutor",
    "created_at": "2023-11-20"
  }
}