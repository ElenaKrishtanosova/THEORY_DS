{
  "quiz_title": "Decision Trees (Default trees)",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Какой основной критерий используется для выбора разбиений в дереве решений?",
      "question_type": "single_choice",
      "options": [
        "Максимизация точности на обучающих данных",
        "Минимизация ошибки классификации/регрессии",
        "Максимизация информативности разбиения (Information Gain/Gini)",
        "Минимизация глубины дерева"
      ],
      "correct_answer": 2,
      "explanation": "Деревья решений выбирают разбиения, которые максимально увеличивают информативность (снижают неопределенность), используя критерии вроде Information Gain или Gini Impurity."
    },
    {
      "question_id": 2,
      "question_text": "Какие методы помогают предотвратить переобучение в деревьях решений? (Выберите 3)",
      "question_type": "multi_choice",
      "options": [
        "Ограничение максимальной глубины дерева",
        "Установка минимального числа образцов в листе",
        "Использование всех признаков для каждого разбиения",
        "Обрезка (pruning) после построения дерева",
        "Увеличение количества разбиений"
      ],
      "correct_answers": [0, 1, 3],
      "explanation": "Основные методы борьбы с переобучением: ограничение глубины, min_samples_leaf, обрезка. Использование всех признаков и больше разбиений может усилить переобучение."
    },
    {
      "question_id": 3,
      "question_text": "В чем разница между Gini Impurity и Information Gain?",
      "question_type": "single_choice",
      "options": [
        "Gini быстрее вычисляется, но дает аналогичные результаты",
        "Information Gain всегда работает лучше",
        "Gini можно использовать только для бинарной классификации",
        "Разницы нет, это одно и то же"
      ],
      "correct_answer": 0,
      "explanation": "Gini Impurity (1-Σp_i²) вычисляется быстрее, чем Information Gain (использует логарифмы), но на практике часто дает схожие результаты."
    },
    {
      "question_id": 4,
      "question_text": "Какие утверждения о деревьях решений верны? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Могут работать как с числовыми, так и с категориальными признаками",
        "Требуют нормализации данных",
        "Чувствительны к монотонным преобразованиям признаков",
        "Строят интерпретируемые модели",
        "Всегда находят глобально оптимальное решение"
      ],
      "correct_answers": [0, 3],
      "explanation": "Деревья: 1) Работают с разными типами данных 2) Создают интерпретируемые модели. Не требуют нормализации, инвариантны к монотонным преобразованиям, ищут локальные оптимумы."
    },
    {
      "question_id": 5,
      "question_text": "Что произойдет, если установить max_depth=None?",
      "question_type": "single_choice",
      "options": [
        "Дерево будет расти, пока не разделит все обучающие данные",
        "Дерево остановится после первого разбиения",
        "Алгоритм будет использовать только линейные разделения",
        "Дерево станет шире, но не глубже"
      ],
      "correct_answer": 0,
      "explanation": "max_depth=None позволяет дереву расти до полного разделения данных или пока не сработают другие ограничения (min_samples_split и т.д.)."
    },
    {
      "question_id": 6,
      "question_text": "Почему деревья решений чувствительны к небольшим изменениям данных?",
      "question_type": "single_choice",
      "options": [
        "Из-за жадного алгоритма построения",
        "Потому что используют только часть признаков",
        "Из-за обязательной нормализации данных",
        "Потому что работают только с линейными зависимостями"
      ],
      "correct_answer": 0,
      "explanation": "Жадный алгоритм (выбор локально оптимальных разбиений) делает деревья чувствительными к изменениям данных - небольшие изменения могут привести к совершенно другой структуре."
    },
    {
      "question_id": 7,
      "question_text": "Как обрабатываются пропущенные значения в деревьях решений?",
      "question_type": "single_choice",
      "options": [
        "Удаляются все строки с пропусками",
        "Заменяются средним/модальным значением",
        "Могут быть учтены через surrogate splits",
        "Деревья не могут работать с пропущенными значениями"
      ],
      "correct_answer": 2,
      "explanation": "Некоторые реализации (например, CART) используют surrogate splits - альтернативные правила разбиения для обработки пропущенных значений."
    },
    {
      "question_id": 8,
      "question_text": "Какие преимущества у деревьев решений? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Не требуют предобработки данных",
        "Автоматически учитывают взаимодействия признаков",
        "Всегда дают лучшую точность, чем другие алгоритмы",
        "Работают быстрее нейросетей на любых данных",
        "Не требуют настройки гиперпараметров"
      ],
      "correct_answers": [0, 1],
      "explanation": "Основные плюсы: 1) Минимальная предобработка (не нужна нормализация) 2) Улавливают сложные взаимодействия признаков. Неверно: не всегда лучшие, скорость зависит от данных, гиперпараметры важны."
    },
    {
      "question_id": 9,
      "question_text": "Что делает параметр min_samples_leaf?",
      "question_type": "single_choice",
      "options": [
        "Ограничивает минимальное число образцов в терминальном узле",
        "Задает максимальную глубину дерева",
        "Определяет число признаков для рассмотрения",
        "Контролирует порог для разделения узла"
      ],
      "correct_answer": 0,
      "explanation": "min_samples_leaf задает минимальное количество образцов, которое должно находиться в листе. Помогает бороться с переобучением."
    },
    {
      "question_id": 10,
      "question_text": "Почему случайный лес лучше одного дерева? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Уменьшает переобучение за счет усреднения",
        "Каждое дерево обучается на всех признаках",
        "Использует только линейные комбинации признаков",
        "Вводит случайность при выборе признаков для разбиений",
        "Требует меньше вычислительных ресурсов"
      ],
      "correct_answers": [0, 3],
      "explanation": "Случайный лес лучше благодаря: 1) Бэггингу (усреднению предсказаний) 2) Случайному выбору признаков. Неверно: деревья используют подмножества признаков, работают с нелинейными границами, требуют больше ресурсов."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1evEorZ3RQlRjpWt6jM9-dCYirfICXCsJ/view?usp=sharing",
    "https://drive.google.com/file/d/1PlE6LmzD5bZwbi0Pqs1B_LflhejLn4xZ/view?usp=sharing",
    "https://drive.google.com/file/d/1UZVxqR3cLSc2QXZDPuuHIiPwzC1044nx/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "intermediate",
    "tags": ["decision_trees", "split_criteria", "pruning", "regularization"],
    "author": "AI Tutor",
    "created_at": "2023-11-20"
  }
}