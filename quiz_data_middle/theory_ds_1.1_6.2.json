{
  "quiz_title": "Dimensionality reduction (PCA)",
  "questions": [
    {
      "question_id": 1,
      "question_text": "Почему важно масштабировать данные перед применением PCA?",
      "question_type": "single_choice",
      "options": [
        "Чтобы алгоритм работал быстрее",
        "Чтобы признаки с большими значениями не доминировали",
        "Это требуется только для категориальных данных",
        "PCA не требует масштабирования"
      ],
      "correct_answer": 1,
      "explanation": "Масштабирование необходимо, чтобы признаки с большим диапазоном значений (например, доход) не оказывали чрезмерного влияния на результат по сравнению с признаками с малым диапазоном (например, возраст)."
    },
    {
      "question_id": 2,
      "question_text": "Как определить важность главных компонент в PCA?",
      "question_type": "single_choice",
      "options": [
        "По величине собственных значений (eigenvalues)",
        "По количеству исходных признаков в компоненте",
        "По времени вычисления компоненты",
        "По корреляции с целевой переменной"
      ],
      "correct_answer": 0,
      "explanation": "Собственные значения показывают, какую долю общей дисперсии данных объясняет каждая главная компонента. Чем больше значение, тем важнее компонента."
    },
    {
      "question_id": 3,
      "question_text": "Что означает, что главные компоненты ортогональны?",
      "question_type": "single_choice",
      "options": [
        "Они содержат разные исходные признаки",
        "Они не коррелируют между собой",
        "Они объясняют одинаковую долю дисперсии",
        "Их всегда ровно две"
      ],
      "correct_answer": 1,
      "explanation": "Ортогональность означает, что главные компоненты статистически независимы - корреляция между любыми двумя разными компонентами равна нулю."
    },
    {
      "question_id": 4,
      "question_text": "Как выбрать количество главных компонент для сохранения?",
      "question_type": "single_choice",
      "options": [
        "Всегда брать первые 2 компоненты",
        "Использовать правило 'локтя' на графике объясненной дисперсии",
        "Брать ровно половину от исходного числа признаков",
        "Выбирать случайным образом"
      ],
      "correct_answer": 1,
      "explanation": "Метод 'локтя' предполагает выбор числа компонент в точке, где добавление новых компонент уже не дает существенного прироста объясненной дисперсии."
    },
    {
      "question_id": 5,
      "question_text": "Какие из этих утверждений о PCA верны? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "PCA уменьшает размерность, сохраняя максимальную дисперсию",
        "Главные компоненты легко интерпретируются",
        "PCA работает только с числовыми данными",
        "PCA - это supervised метод"
      ],
      "correct_answers": [0, 2],
      "explanation": "PCA действительно сохраняет максимальную дисперсию и требует числовых данных. Главные компоненты часто трудно интерпретировать, и PCA является unsupervised методом."
    },
    {
      "question_id": 6,
      "question_text": "Почему PCA плохо работает с категориальными признаками?",
      "question_type": "single_choice",
      "options": [
        "PCA не может обрабатывать текст",
        "Категории не имеют числового представления",
        "PCA основан на ковариации, которая требует числовых данных",
        "Все варианты верны"
      ],
      "correct_answer": 2,
      "explanation": "PCA использует ковариационную матрицу, которая может быть вычислена только для числовых данных. Категориальные признаки нужно предварительно преобразовать."
    },
    {
      "question_id": 7,
      "question_text": "Как влияют выбросы на результаты PCA?",
      "question_type": "single_choice",
      "options": [
        "Не влияют, если данных много",
        "Могут существенно исказить направление главных компонент",
        "Улучшают качество уменьшения размерности",
        "Автоматически удаляются алгоритмом"
      ],
      "correct_answer": 1,
      "explanation": "Выбросы сильно влияют на PCA, так как алгоритм чувствителен к дисперсии. Один выброс может значительно изменить направление главных компонент."
    },
    {
      "question_id": 8,
      "question_text": "Что произойдет, если применить PCA к полностью коррелированным признакам?",
      "question_type": "single_choice",
      "options": [
        "PCA создаст одну компоненту для коррелированной группы",
        "Получим ошибку вычисления",
        "Все компоненты будут одинаково важны",
        "PCA не работает с коррелированными данными"
      ],
      "correct_answer": 0,
      "explanation": "PCA автоматически обнаружит полную корреляцию и создаст одну компоненту для группы коррелированных признаков, исключив избыточность."
    },
    {
      "question_id": 9,
      "question_text": "Как можно использовать PCA в машинном обучении? (Выберите 2)",
      "question_type": "multi_choice",
      "options": [
        "Для визуализации многомерных данных",
        "Как замену feature selection",
        "Для увеличения числа признаков",
        "Для ускорения работы алгоритмов"
      ],
      "correct_answers": [0, 3],
      "explanation": "PCA часто используют для визуализации (снижая размерность до 2-3 компонент) и для ускорения алгоритмов за счет уменьшения числа признаков."
    },
    {
      "question_id": 10,
      "question_text": "Какое из этих утверждений о применении PCA верно?",
      "question_type": "single_choice",
      "options": [
        "PCA всегда улучшает качество моделей",
        "PCA следует применять после разделения на train/test",
        "PCA можно использовать для обработки пропущенных значений",
        "PCA заменяет необходимость feature engineering"
      ],
      "correct_answer": 1,
      "explanation": "PCA нужно обучать только на тренировочных данных, чтобы избежать 'утечки' информации. Его применение не гарантирует улучшения качества и не заменяет feature engineering."
    }
  ],
  "pdf_links": [
    "https://drive.google.com/file/d/1aqutpmoJmy6x_TuXtJa2WCEktbYZ0iNL/view?usp=sharing",
    "https://drive.google.com/file/d/1l9iaDy6Rhg4S8xffyW6A1h1Pk-fZ9ELS/view?usp=sharing"
  ],
  "metadata": {
    "difficulty_level": "intermediate",
    "tags": ["PCA", "dimensionality_reduction", "feature_scaling", "machine_learning"],
    "author": "Data Science Instructor",
    "created_at": "2023-11-25"
  }
}